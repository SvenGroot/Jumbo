using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using Tkl.Jumbo.IO;
using System.Reflection;
using Tkl.Jumbo.Dfs;
using Tkl.Jumbo.Jet.Channels;
using System.IO;
using System.Reflection.Emit;
using Tkl.Jumbo.Jet.Tasks;
using System.Diagnostics;

namespace Tkl.Jumbo.Jet.Jobs
{
    /// <summary>
    /// Delegate for tasks.
    /// </summary>
    /// <typeparam name="TInput">The type of the input records.</typeparam>
    /// <typeparam name="TOutput">The type of the output records.</typeparam>
    /// <param name="input">The record reader providing the input records.</param>
    /// <param name="output">The record writer collecting the output records.</param>
    public delegate void TaskFunction<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output) where TInput : IWritable, new() where TOutput : IWritable, new();

    /// <summary>
    /// Delegate for accumulator tasks
    /// </summary>
    /// <typeparam name="TKey">The type of the keys.</typeparam>
    /// <typeparam name="TValue">The type of the values.</typeparam>
    /// <param name="key">The key of the record.</param>
    /// <param name="value">The value associated with the key in the accumulator that must be updated.</param>
    /// <param name="newValue">The new value associated with the key.</param>
    public delegate void AccumulatorFunction<TKey, TValue>(TKey key, TValue value, TValue newValue) where TKey : IWritable, IComparable<TKey>, new() where TValue : class, IWritable, new();

    /// <summary>
    /// Provides easy construction of Jumbo Jet jobs.
    /// </summary>
    public sealed class JobBuilder
    {
        #region Nested types

        private sealed class RecordReaderReference<T> : RecordReader<T>
            where T : IWritable, new()
        {
            public RecordReaderReference(JobBuilder jobBuilder, string input, Type recordReaderType)
            {
                JobBuilder = jobBuilder;
                Input = input;
                RecordReaderType = recordReaderType;
            }

            public Type RecordReaderType { get; private set; }

            public string Input { get; private set; }

            public JobBuilder JobBuilder { get; private set; }

            public override float Progress
            {
                get { throw new NotSupportedException(); }
            }

            protected override bool ReadRecordInternal()
            {
                throw new NotSupportedException();
            }
        }

        private sealed class RecordWriterReference<T> : RecordWriter<T>
            where T : IWritable, new()
        {
            public RecordWriterReference(JobBuilder jobBuilder, string output, Type recordWriterType, int blockSize, int replicationFactor)
            {
                JobBuilder = jobBuilder;
                Output = output;
                RecordWriterType = recordWriterType;
                BlockSize = blockSize;
                ReplicationFactor = replicationFactor;
            }

            public string Output { get; private set; }

            public Type RecordWriterType { get; private set; }

            public JobBuilder JobBuilder { get; private set; }

            public int BlockSize { get; private set; }

            public int ReplicationFactor { get; private set; }

            protected override void WriteRecordInternal(T record)
            {
                throw new NotSupportedException();
            }
        }

        #endregion

        private readonly JobConfiguration _job = new JobConfiguration();
        private readonly HashSet<Assembly> _assemblies = new HashSet<Assembly>();
        private readonly DfsClient _dfsClient;
        private readonly JetClient _jetClient;
        private AssemblyBuilder _dynamicAssembly;
        private ModuleBuilder _dynamicModule;
        private string _dynamicAssemblyDir;
        private bool _assemblySaved;
        private int _sortStages; // counter used to disambiguate IDs of empty stages generated by SortRecords if needed.

        /// <summary>
        /// Initializes a new instance of the <see cref="JobBuilder"/> class.
        /// </summary>
        public JobBuilder()
            : this(null, null)
        {
        }

        /// <summary>
        /// Initializes a new instance of the <see cref="JobBuilder"/> class with the specified DFS and Jet clients.
        /// </summary>
        /// <param name="dfsClient">The DFS client to use, or <see langword="null"/> to create one using the default configuration.</param>
        /// <param name="jetClient">The Jet client to use, or <see langword="null"/> to create one using the default configuration.</param>
        public JobBuilder(DfsClient dfsClient, JetClient jetClient)
        {
            _dfsClient = dfsClient ?? new DfsClient();
            _jetClient = jetClient ?? new JetClient();
        }

        /// <summary>
        /// Gets the job configuration.
        /// </summary>
        public JobConfiguration JobConfiguration
        {
            get
            {
                _job.AssemblyFileNames.Clear();
                _job.AssemblyFileNames.AddRange(from a in Assemblies select Path.GetFileName(a.Location));
                if( _dynamicAssembly != null )
                    _job.AssemblyFileNames.Add(_dynamicAssembly.GetName().Name + ".dll");

                return _job;
            }
        }

        /// <summary>
        /// Gets the full paths of all the assembly files used by this job builder's job.
        /// </summary>
        public IEnumerable<string> AssemblyFiles
        {
            get
            {
                var files = from a in Assemblies
                            select a.Location;
                if( _dynamicAssembly != null )
                {
                    SaveDynamicAssembly();
                    string assemblyFileName = _dynamicAssembly.GetName().Name + ".dll";
                    files = files.Concat(new[] { Path.Combine(_dynamicAssemblyDir, assemblyFileName) });
                }
                return files;
            }
        }

        private IEnumerable<Assembly> Assemblies
        {
            get 
            {
                _assemblies.Remove(typeof(BasicJob).Assembly); // Don't include Tkl.Jumbo.Jet assembly
                _assemblies.Remove(typeof(RecordReader<>).Assembly); // Don't include Tkl.Jumbo assembly
                return _assemblies; 
            }
        }

        /// <summary>
        /// Creates a record reader that reads data from the specified input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input file or directory on the DFS to read from.</param>
        /// <param name="recordReaderType">The type of the record reader to use.</param>
        /// <returns>An instance of a record reader. Note the return value is not necessarily of the type specified in <paramref name="recordReaderType"/>,
        /// so do not try to cast it.</returns>
        public RecordReader<T> CreateRecordReader<T>(string input, Type recordReaderType)
            where T : IWritable, new()
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( recordReaderType == null )
                throw new ArgumentNullException("recordReaderType");
            if( !recordReaderType.IsSubclassOf(typeof(RecordReader<T>)) )
                throw new ArgumentException(string.Format(System.Globalization.CultureInfo.CurrentCulture, "recordReaderType does not specify a type that inherits from {0}", typeof(RecordReader<T>).FullName), "recordReaderType");

            return new RecordReaderReference<T>(this, input, recordReaderType);
        }


        /// <summary>
        /// Creates a record reader that writes data to the specified input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The directory on the DFS to write to.</param>
        /// <param name="recordWriterType">The type of the record writer to use.</param>
        /// <returns>An instance of a record writer. Note the return value is not necessarily of the type specified in <paramref name="recordWriterType"/>,
        /// so do not try to cast it.</returns>
        public RecordWriter<T> CreateRecordWriter<T>(string output, Type recordWriterType)
            where T : IWritable, new()
        {
            return CreateRecordWriter<T>(output, recordWriterType, 0, 0);
        }

        /// <summary>
        /// Creates a record reader that writes data to the specified input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The directory on the DFS to write to.</param>
        /// <param name="recordWriterType">The type of the record writer to use.</param>
        /// <param name="blockSize">The block size of the output files, or 0 to use the DFS default block size.</param>
        /// <param name="replicationFactor">The replication factor of the output files, or 0 to use the DFS default replication factor.</param>
        /// <returns>An instance of a record writer. Note the return value is not necessarily of the type specified in <paramref name="recordWriterType"/>,
        /// so do not try to cast it.</returns>
        public RecordWriter<T> CreateRecordWriter<T>(string output, Type recordWriterType, int blockSize, int replicationFactor)
            where T : IWritable, new()
        {
            if( output == null )
                throw new ArgumentNullException("output");
            if( recordWriterType == null )
                throw new ArgumentNullException("recordWriterType");
            if( !recordWriterType.IsSubclassOf(typeof(RecordWriter<T>)) )
                throw new ArgumentException(string.Format(System.Globalization.CultureInfo.CurrentCulture, "recordWriterType does not specify a type that inherits from {0}", typeof(RecordWriter<T>).FullName), "recordReaderType");
            if( replicationFactor < 0 )
                throw new InvalidOperationException("Replication factor may not be less than zero.");
            if( blockSize < 0 )
                throw new InvalidOperationException("Block size may not be less than zero.");

            return new RecordWriterReference<T>(this, output, recordWriterType, blockSize, replicationFactor);
        }

        /// <summary>
        /// Processes records using the specified task type.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="taskType">The type of the task.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, Type taskType)
            where TInput : IWritable, new()
            where TOutput : IWritable, new()
        {
            ProcessRecords(input, output, taskType, null);
        }

            /// <summary>
        /// Processes records using the specified task type.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="taskType">The type of the task.</param>
        /// <param name="stageId">The ID of this processing stage in the job, or <see langword="null"/> to use the name of the task type.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, Type taskType, string stageId)
            where TInput : IWritable, new()
            where TOutput : IWritable, new()
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( taskType == null )
                throw new ArgumentNullException("taskType");

            if( stageId == null )
                stageId = taskType.Name;

            Type taskInterfaceType = taskType.FindGenericInterfaceType(typeof(ITask<,>), true);
            Type[] arguments = taskInterfaceType.GetGenericArguments();
            if( !(arguments[0] == typeof(TInput) && arguments[1] == typeof(TOutput)) )
                throw new ArgumentException("The specified task type does not have input and output types matching the specified record reader and writer.", "taskType");

            string outputPath = null;
            Type outputWriterType = null;
            RecordCollector<TOutput> collector = null;
            RecordWriterReference<TOutput> outputRef = output as RecordWriterReference<TOutput>;
            if( outputRef != null )
            {
                outputPath = outputRef.Output;
                outputWriterType = outputRef.RecordWriterType;
                _assemblies.Add(outputWriterType.Assembly);
            }
            else
            {
                collector = RecordCollector<TOutput>.GetCollector(output);
                if( collector != null )
                {
                    if( collector.InputStage != null || collector.InputChannels != null )
                        throw new ArgumentException("Cannot write to the specified record writer, because that writer is already used by another stage.", "output");
                    _assemblies.Add(collector.PartitionerType.Assembly);
                }
                else
                    throw new ArgumentException("Unsupported output record writer.", "output");
            }

            RecordReaderReference<TInput> inputRef = input as RecordReaderReference<TInput>;
            StageConfiguration stage;
            if( inputRef != null )
            {
                // We're adding an input stage.
                stage = _job.AddInputStage(stageId, _dfsClient.NameServer.GetFileSystemEntryInfo(inputRef.Input), taskType, inputRef.RecordReaderType, outputPath, outputWriterType);
                _assemblies.Add(inputRef.RecordReaderType.Assembly);
            }
            else
            {
                RecordCollector<TInput> inputCollector = RecordCollector<TInput>.GetCollector(input);
                if( inputCollector == null )
                    throw new ArgumentException("The specified record reader was not created by a JobBuilder or RecordCollector.", "input");
                if( inputCollector.InputStage == null && inputCollector.InputChannels == null )
                    throw new ArgumentException("Cannot read from the specified record reader because the associated RecordCollector isn't being written to.");

                // Determine the number of partitions to use on the input channel
                int taskCount;
                if( inputCollector.Partitions != null )
                    taskCount = inputCollector.Partitions.Value; // Use specified amount
                else if( inputCollector.ChannelType == ChannelType.Pipeline )
                    taskCount = 1; // Pipeline channel always uses one if unspecified
                else if( inputCollector.InputStage != null && inputCollector.InputStage.InternalPartitionCount > 1 )
                    taskCount = inputCollector.InputStage.InternalPartitionCount; // Connecting to a compound stage with internal partitioning we must use the same number of tasks.
                else
                    taskCount = _jetClient.JobServer.GetMetrics().TaskServers.Count; // Otherwise default to the number of nodes in the cluster

                // We can replace an empty task if:
                // - The channel type is pipeline and taskCount is one.
                // - The channel type is not specified, the input stage is not a child stage, and the task count, partitioner type and multi input record 
                //   reader type are the same as the EmptyTask's input.
                if( inputCollector.InputStage != null && inputCollector.InputStage.TaskType == typeof(EmptyTask<TInput>) &&
                    ((inputCollector.ChannelType == ChannelType.Pipeline && taskCount == 1) ||
                     (inputCollector.ChannelType == null && inputCollector.InputStage.Parent == null && taskCount == inputCollector.InputStage.TaskCount && MatchInputChannelSettings(inputCollector.InputStage, inputCollector.PartitionerType, inputCollector.MultiInputRecordReaderType))) )
                {
                    // Replace the EmptyTask
                    stage = inputCollector.InputStage;
                    stage.TaskType = taskType;
                    _job.RenameStage(stage, stageId);
                    if( outputRef != null )
                    {
                        stage.SetDfsOutput(outputPath, outputWriterType);
                    }
                }
                else
                {
                    // We default to the File channel if not specified.
                    ChannelType channelType = inputCollector.ChannelType == null ? ChannelType.File : inputCollector.ChannelType.Value;

                    InputStageInfo[] inputStages;
                    if( inputCollector.InputStage != null )
                    {
                        inputStages = new[] { new InputStageInfo(inputCollector.InputStage)
                        {
                            ChannelType = channelType,
                            MultiInputRecordReaderType = inputCollector.MultiInputRecordReaderType,
                            PartitionerType = inputCollector.PartitionerType
                        } };
                    }
                    else
                        inputStages = inputCollector.InputChannels;
                    // If there is only one input, the stageMultiInputRecordReaderType parameter won't be used so it doesn't matter what we pass.
                    // If there are more than one, the multi input record reader type of this inputCollector indicates what reader to use to combine
                    // the input from the stages so we must pass that.
                    stage = _job.AddStage(stageId, taskType, taskCount, inputStages, inputCollector.MultiInputRecordReaderType, outputPath, outputWriterType);
                }
            }

            if( outputRef != null )
            {
                stage.DfsOutput.BlockSize = outputRef.BlockSize;
                stage.DfsOutput.ReplicationFactor = outputRef.ReplicationFactor;
            }

            if( collector != null )
                collector.InputStage = stage;

            if( !object.Equals(taskType.Assembly, _dynamicAssembly) )
                _assemblies.Add(taskType.Assembly);
        }

        /// <summary>
        /// Processes records using the specified task function.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="task">The task function.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, TaskFunction<TInput, TOutput> task)
            where TInput : IWritable, new()
            where TOutput : IWritable, new()
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( task == null )
                throw new ArgumentNullException("task");

            MethodInfo taskMethod = task.Method;
            if( !(taskMethod.IsStatic && taskMethod.IsPublic) )
                throw new ArgumentException("The task method specified must be public and static.", "task");

            CreateDynamicAssembly();

            TypeBuilder taskTypeBuilder = _dynamicModule.DefineType(_dynamicAssembly.GetName().Name + "." + taskMethod.Name, TypeAttributes.Class | TypeAttributes.Public | TypeAttributes.Sealed, typeof(Configurable), new[] { typeof(IPullTask<TInput, TOutput>) });

            SetAllowRecordReuseAttribute(taskMethod, taskTypeBuilder);

            MethodBuilder runMethod = taskTypeBuilder.DefineMethod("Run", MethodAttributes.Public | MethodAttributes.Virtual, null, new[] { typeof(RecordReader<TInput>), typeof(RecordWriter<TOutput>) });
            runMethod.DefineParameter(1, ParameterAttributes.None, "input");
            runMethod.DefineParameter(2, ParameterAttributes.None, "output");

            ILGenerator generator = runMethod.GetILGenerator();
            generator.Emit(OpCodes.Ldarg_1);
            generator.Emit(OpCodes.Ldarg_2);
            generator.Emit(OpCodes.Call, taskMethod);
            generator.Emit(OpCodes.Ret);

            Type taskType = taskTypeBuilder.CreateType();

            ProcessRecords(input, output, taskType);
        }

        /// <summary>
        /// Processes records using the specified accumulator function.
        /// </summary>
        /// <typeparam name="TKey">The type of the key of the records.</typeparam>
        /// <typeparam name="TValue">The type of the value of the records.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="accumulatorTaskType">The accumulator task type.</param>
        public void AccumulateRecords<TKey, TValue>(RecordReader<KeyValuePairWritable<TKey, TValue>> input, RecordWriter<KeyValuePairWritable<TKey, TValue>> output, Type accumulatorTaskType)
            where TKey : IWritable, IComparable<TKey>, new()
            where TValue : class, IWritable, new()
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( accumulatorTaskType == null )
                throw new ArgumentNullException("accumulatorTaskType");
            if( !accumulatorTaskType.IsSubclassOf(typeof(AccumulatorTask<TKey, TValue>)) )
                throw new ArgumentException("The specified task type is not an accumulator task.", "accumulatorTaskType");

            // DFS input is treated as being a single range; therefore to correctly accumulate the entire input, we must partition it.
            // If connecting directly to DFS input, we will therefore gather all into a single partition.
            // We will always connect an accumulator task to the input. If the input is another task, we pipeline it.

            RecordCollector<KeyValuePairWritable<TKey, TValue>> collector = RecordCollector<KeyValuePairWritable<TKey, TValue>>.GetCollector(input);
            if( collector == null )
            {
                // TODO: Match partitions if channel output (see how that works out with pipeline output channel; accumulate too)
                // Connecting directly to DFS input, so we're creating a single partition.
                RecordCollector<KeyValuePairWritable<TKey, TValue>> intermediateCollector = new RecordCollector<KeyValuePairWritable<TKey, TValue>>(null, null, 1);
                ProcessRecords(input, intermediateCollector.CreateRecordWriter(), accumulatorTaskType, "Input" + accumulatorTaskType.Name);
                // TODO: Optimize for input with only one block (in which case this second step isn't necessary).
                ProcessRecords(intermediateCollector.CreateRecordReader(), output, accumulatorTaskType);
            }
            else
            {
                if( collector.InputStage != null )
                {
                    RecordWriter<KeyValuePairWritable<TKey, TValue>> outputWriter = output;
                    RecordCollector<KeyValuePairWritable<TKey, TValue>> intermediateCollector = null;
                    // We'll need a second step unless:
                    // - The input channel is explicitly a pipeline channel.
                    // - There is only one input task and the output is to the DFS (in this case, if partitioning was specified it'll use internal partitioning)
                    //   We use a second step when writing to a channel because we don't want to use internal partitioning in this case.
                    //   Note we use a second step when writing to a channel even if there is only one input task and no partitioning.The second step will use
                    //   EmptyTask (no aggregation needed with one input) so it can be replaced later.
                    if( !(collector.ChannelType == ChannelType.Pipeline || (collector.InputStage.TaskCount == 1 && output is RecordWriterReference<KeyValuePairWritable<TKey, TValue>>)) )
                    {
                        // We'll need a second step, so create an intermedate collector and modify the original input channel
                        intermediateCollector = new RecordCollector<KeyValuePairWritable<TKey, TValue>>(collector.ChannelType, collector.PartitionerType, collector.Partitions);
                        collector.Partitions = 1;
                        collector.ChannelType = ChannelType.Pipeline;
                        outputWriter = intermediateCollector.CreateRecordWriter();
                    }
                    else if( collector.ChannelType == null )
                    {
                        // If no second stage is needed and the channel type is unspecified, make it a pipeline channel.
                        collector.ChannelType = ChannelType.Pipeline;
                    }

                    // Create the pipelined stage (or if the input stage had one task, and the channel type was specified to be other than pipeline, that stage)
                    ProcessRecords(input, outputWriter, accumulatorTaskType, "Input" + accumulatorTaskType.Name); // Have to specify name because it might replace rather than pipeline

                    if( intermediateCollector != null )
                    {
                        // Create the second step.
                        // If the input stage has only one task, then we don't need to aggregate the results so we can use EmptyTask. 
                        Type taskType = collector.InputStage.TaskCount == 1 ? typeof(EmptyTask<KeyValuePairWritable<TKey, TValue>>) : accumulatorTaskType;
                        ProcessRecords(intermediateCollector.CreateRecordReader(), output, taskType);
                    }
                }
                else if( collector.InputChannels != null )
                {
                    // TODO: Reconsider this. Like files, are join results considered single range or pre-partitioned?
                    // We're connecting to multiple input channels. We will leave the partitions as they are, so it's only a single-step, and we can't pipeline.
                    ProcessRecords(input, output, accumulatorTaskType);
                }
                else
                    throw new ArgumentException("The specified record reader was not created by a JobBuilder or RecordCollector.", "input");
            }
        }

        /// <summary>
        /// Processes records using the specified accumulator function.
        /// </summary>
        /// <typeparam name="TKey">The type of the key of the records.</typeparam>
        /// <typeparam name="TValue">The type of the value of the records.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="accumulator">The accumulator function.</param>
        public void AccumulateRecords<TKey, TValue>(RecordReader<KeyValuePairWritable<TKey, TValue>> input, RecordWriter<KeyValuePairWritable<TKey, TValue>> output, AccumulatorFunction<TKey, TValue> accumulator)
            where TKey : IWritable, IComparable<TKey>, new()
            where TValue : class, IWritable, new()
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( accumulator == null )
                throw new ArgumentNullException("task");

            MethodInfo accumulatorMethod = accumulator.Method;
            if( !(accumulatorMethod.IsStatic && accumulatorMethod.IsPublic) )
                throw new ArgumentException("The accumulator method specified must be public and static.", "task");

            CreateDynamicAssembly();

            TypeBuilder taskTypeBuilder = _dynamicModule.DefineType(_dynamicAssembly.GetName().Name + "." + accumulatorMethod.Name, TypeAttributes.Class | TypeAttributes.Sealed, typeof(AccumulatorTask<TKey, TValue>));

            SetAllowRecordReuseAttribute(accumulatorMethod, taskTypeBuilder);

            MethodBuilder accumulateMethod = taskTypeBuilder.DefineMethod("Accumulate", MethodAttributes.Public | MethodAttributes.Virtual, null, new[] { typeof(TKey), typeof(TValue), typeof(TValue) });
            accumulateMethod.DefineParameter(1, ParameterAttributes.None, "key");
            accumulateMethod.DefineParameter(2, ParameterAttributes.None, "value");
            accumulateMethod.DefineParameter(3, ParameterAttributes.None, "newValue");

            ILGenerator generator = accumulateMethod.GetILGenerator();
            generator.Emit(OpCodes.Ldarg_1);
            generator.Emit(OpCodes.Ldarg_2);
            generator.Emit(OpCodes.Ldarg_3);
            generator.Emit(OpCodes.Call, accumulatorMethod);
            generator.Emit(OpCodes.Ret);

            Type taskType = taskTypeBuilder.CreateType();

            AccumulateRecords(input, output, taskType);
        }

        /// <summary>
        /// Sorts the records using the specified partitioner type.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input records to sort.</param>
        /// <param name="output">The record writer receiving the sorted result.</param>
        /// <remarks>
        /// <para>
        ///   If <paramref name="input"/> or <paramref name="output"/> were created by a <see cref="RecordCollector{T}"/>, the partitioner type
        ///   and number of partitions specified by those collectors will not be used.
        /// </para>
        /// </remarks>
        public void SortRecords<T>(RecordReader<T> input, RecordWriter<T> output)
            where T : IWritable, new()
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");

            RecordCollector<T> inputCollector = RecordCollector<T>.GetCollector(input);

            Type sortTaskType = typeof(SortTask<T>);
            Type mergeReaderType = typeof(MergeRecordReader<T>);

            // TODO: Stage names.
            // DFS input is treated as being a single range; therefore to correctly sort the entire input, we must partition it.
            // If connecting directly to DFS input, we will therefore gather all into a single partition.
            // We will always connect an accumulator task to the input. If the input is another task, we pipeline it.

            if( inputCollector == null )
            {
                // TODO: Match partitions if channel output (see how that works out with pipeline output channel; accumulate too; note if more than 1 partition needs EmptyTask first)
                // Input is DFS. We will sort it, then merge
                RecordCollector<T> intermediateCollector = new RecordCollector<T>(null, null, 1);
                intermediateCollector.MultiInputRecordReaderType = mergeReaderType;
                ProcessRecords(input, intermediateCollector.CreateRecordWriter(), sortTaskType);
                // We need an empty task that merges the result and writes to the output.
                ProcessRecords(intermediateCollector.CreateRecordReader(), output, typeof(EmptyTask<T>));
            }
            else
            {
                if( inputCollector.InputStage != null )
                {
                    RecordWriter<T> outputWriter = output;
                    RecordCollector<T> intermediateCollector = null;
                    // We won't need a second stage if the channel is explicitly pipeline or if we have only one input.
                    // Unlike Accumulate, we don't need a second stage with one input when writing to a channel because we
                    // always use internal partitioning anyway.
                    if( !(inputCollector.ChannelType == ChannelType.Pipeline || inputCollector.InputStage.TaskCount == 1) )
                    {
                        intermediateCollector = new RecordCollector<T>(inputCollector.ChannelType, inputCollector.PartitionerType, inputCollector.Partitions);
                        intermediateCollector.MultiInputRecordReaderType = mergeReaderType;
                        inputCollector.ChannelType = ChannelType.Pipeline;
                        outputWriter = intermediateCollector.CreateRecordWriter();
                    }
                    else if( inputCollector.ChannelType == null )
                    {
                        // Channel type was not specified, so use Pipeline
                        inputCollector.ChannelType = ChannelType.Pipeline;
                    }
                    // TODO: Match partitions if channel output (see how that works out with pipeline output channel)

                    ProcessRecords(input, outputWriter, sortTaskType);

                    if( intermediateCollector != null )
                        ProcessRecords(intermediateCollector.CreateRecordReader(), output, typeof(EmptyTask<T>));
                }
                else if( inputCollector.InputChannels == null )
                {
                    // TODO: This'll need to work much like the DFS input case. Or maybe not; see Accumulate.
                }
                else
                    throw new ArgumentException("The specified record reader was not created by a JobBuilder or RecordCollector.", "input");
            }
        }

        /// <summary>
        /// 
        /// </summary>
        /// <typeparam name="TOuter"></typeparam>
        /// <typeparam name="TInner"></typeparam>
        /// <typeparam name="TResult"></typeparam>
        /// <param name="outerInput"></param>
        /// <param name="innerInput"></param>
        /// <param name="output"></param>
        /// <param name="joinRecordReader"></param>
        /// <param name="outerComparer"></param>
        /// <param name="innerComparer"></param>
        public void Join<TOuter, TInner, TResult>(RecordReader<TOuter> outerInput, RecordReader<TInner> innerInput, RecordWriter<TResult> output, Type joinRecordReader, Type outerComparer, Type innerComparer)
            where TOuter : class, IWritable, new()
            where TInner : class, IWritable, new()
            where TResult : IWritable, new()
        {
            if( outerInput == null )
                throw new ArgumentNullException("outerInput");
            if( innerInput == null )
                throw new ArgumentNullException("innerInput");
            if( output == null )
                throw new ArgumentNullException("output");
            if( joinRecordReader == null )
                throw new ArgumentNullException("joinRecordReader");
            if( !joinRecordReader.IsSubclassOf(typeof(InnerJoinRecordReader<TOuter, TInner, TResult>)) )
                throw new ArgumentException("The specified join record reader type does not inherit from InnerJoinRecordReader.", "joinRecordReader");
            if( outerComparer != null && outerComparer.GetInterfaces().Contains(typeof(IComparer<TOuter>)) )
                throw new ArgumentException("The specified outer comparer does not implement IComparer<TOuter>.", "outerComparer");
            if( innerComparer != null && innerComparer.GetInterfaces().Contains(typeof(IComparer<TInner>)) )
                throw new ArgumentException("The specified inner comparer does not implement IComparer<TInner>.", "innerComparer");

            
        }

        private static void SetAllowRecordReuseAttribute(MethodInfo taskMethod, TypeBuilder taskTypeBuilder)
        {
            Type allowRecordReuseAttributeType = typeof(AllowRecordReuseAttribute);
            AllowRecordReuseAttribute allowRecordReuse = (AllowRecordReuseAttribute)Attribute.GetCustomAttribute(taskMethod, allowRecordReuseAttributeType);
            if( allowRecordReuse != null )
            {
                ConstructorInfo ctor = allowRecordReuseAttributeType.GetConstructor(Type.EmptyTypes);
                PropertyInfo passThrough = allowRecordReuseAttributeType.GetProperty("PassThrough");

                CustomAttributeBuilder allowRecordReuseBuilder = new CustomAttributeBuilder(ctor, new object[] { }, new[] { passThrough }, new object[] { allowRecordReuse.PassThrough });
                taskTypeBuilder.SetCustomAttribute(allowRecordReuseBuilder);
            }
        }

        private void SaveDynamicAssembly()
        {
            string assemblyFileName = _dynamicAssembly.GetName().Name + ".dll";
            _dynamicAssembly.Save(assemblyFileName);
            _assemblySaved = true;
        }

        private void CreateDynamicAssembly()
        {
            if( _assemblySaved )
                throw new InvalidOperationException("You cannot define new delegate-based tasks after the dynamic assembly has been saved.");
            if( _dynamicAssembly == null )
            {
                // Use a Guid to ensure a unique name.
                AssemblyName name = new AssemblyName("Tkl.Jumbo.Jet.Generated." + Guid.NewGuid().ToString("N"));
                _dynamicAssemblyDir = Path.GetTempPath();
                _dynamicAssembly = AppDomain.CurrentDomain.DefineDynamicAssembly(name, AssemblyBuilderAccess.Save, _dynamicAssemblyDir);
                _dynamicModule = _dynamicAssembly.DefineDynamicModule(name.Name, name.Name + ".dll");
            }
        }

        private bool MatchInputChannelSettings(StageConfiguration stage, Type partitionerType, Type multiInputRecordReaderType)
        {
            var inputStages = _job.GetInputStagesForStage(stage.StageId).ToArray();
            if( inputStages.Length == 1 )
            {
                StageConfiguration inputStage = inputStages[0];
                return inputStage.OutputChannel.PartitionerType.Type == partitionerType && inputStage.OutputChannel.MultiInputRecordReaderType.Type == multiInputRecordReaderType;
            }
            else
                return false;
        }
    }
}
