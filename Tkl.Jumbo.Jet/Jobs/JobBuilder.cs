// $Id$
//
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;
using Tkl.Jumbo.IO;
using System.Reflection;
using Tkl.Jumbo.Dfs;
using Tkl.Jumbo.Jet.Channels;
using System.IO;
using System.Reflection.Emit;
using Tkl.Jumbo.Jet.Tasks;
using System.Diagnostics;

namespace Tkl.Jumbo.Jet.Jobs
{
    /// <summary>
    /// Provides easy construction of Jumbo Jet jobs.
    /// </summary>
    public sealed class JobBuilder
    {
        #region Nested types

        private sealed class RecordReaderReference<T> : RecordReader<T>
        {
            public RecordReaderReference(JobBuilder jobBuilder, string input, Type recordReaderType)
            {
                JobBuilder = jobBuilder;
                Input = input;
                RecordReaderType = recordReaderType;
            }

            public Type RecordReaderType { get; private set; }

            public string Input { get; private set; }

            public JobBuilder JobBuilder { get; private set; }

            public override float Progress
            {
                get { throw new NotSupportedException(); }
            }

            protected override bool ReadRecordInternal()
            {
                throw new NotSupportedException();
            }
        }

        private sealed class RecordWriterReference<T> : RecordWriter<T>
       {
            public RecordWriterReference(JobBuilder jobBuilder, string output, Type recordWriterType, int blockSize, int replicationFactor)
            {
                JobBuilder = jobBuilder;
                Output = output;
                RecordWriterType = recordWriterType;
                BlockSize = blockSize;
                ReplicationFactor = replicationFactor;
            }

            public string Output { get; private set; }

            public Type RecordWriterType { get; private set; }

            public JobBuilder JobBuilder { get; private set; }

            public int BlockSize { get; private set; }

            public int ReplicationFactor { get; private set; }

            protected override void WriteRecordInternal(T record)
            {
                throw new NotSupportedException();
            }
        }

        #endregion

        private readonly JobConfiguration _job = new JobConfiguration();
        private readonly HashSet<Assembly> _assemblies = new HashSet<Assembly>();
        private readonly DfsClient _dfsClient;
        private readonly JetClient _jetClient;
        private AssemblyBuilder _dynamicAssembly;
        private ModuleBuilder _dynamicModule;
        private string _dynamicAssemblyDir;
        private bool _assemblySaved;
        private int _sortStages; // counter used to disambiguate IDs of empty stages generated by SortRecords if needed.

        /// <summary>
        /// Initializes a new instance of the <see cref="JobBuilder"/> class.
        /// </summary>
        public JobBuilder()
            : this(null, null)
        {
        }

        /// <summary>
        /// Initializes a new instance of the <see cref="JobBuilder"/> class with the specified DFS and Jet clients.
        /// </summary>
        /// <param name="dfsClient">The DFS client to use, or <see langword="null"/> to create one using the default configuration.</param>
        /// <param name="jetClient">The Jet client to use, or <see langword="null"/> to create one using the default configuration.</param>
        public JobBuilder(DfsClient dfsClient, JetClient jetClient)
        {
            _dfsClient = dfsClient ?? new DfsClient();
            _jetClient = jetClient ?? new JetClient();
        }

        /// <summary>
        /// Gets the job configuration.
        /// </summary>
        public JobConfiguration JobConfiguration
        {
            get
            {
                _job.AssemblyFileNames.Clear();
                _job.AssemblyFileNames.AddRange(from a in Assemblies select Path.GetFileName(a.Location));
                if( _dynamicAssembly != null )
                    _job.AssemblyFileNames.Add(_dynamicAssembly.GetName().Name + ".dll");

                return _job;
            }
        }

        /// <summary>
        /// Gets the full paths of all the assembly files used by this job builder's job.
        /// </summary>
        public IEnumerable<string> AssemblyFiles
        {
            get
            {
                var files = from a in Assemblies
                            select a.Location;
                if( _dynamicAssembly != null )
                {
                    SaveDynamicAssembly();
                    string assemblyFileName = _dynamicAssembly.GetName().Name + ".dll";
                    files = files.Concat(new[] { Path.Combine(_dynamicAssemblyDir, assemblyFileName) });
                }
                return files;
            }
        }

        private IEnumerable<Assembly> Assemblies
        {
            get 
            {
                _assemblies.Remove(typeof(BasicJob).Assembly); // Don't include Tkl.Jumbo.Jet assembly
                _assemblies.Remove(typeof(RecordReader<>).Assembly); // Don't include Tkl.Jumbo assembly
                return _assemblies; 
            }
        }

        /// <summary>
        /// Creates a record reader that reads data from the specified input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input file or directory on the DFS to read from.</param>
        /// <param name="recordReaderType">The type of the record reader to use.</param>
        /// <returns>An instance of a record reader. Note the return value is not necessarily of the type specified in <paramref name="recordReaderType"/>,
        /// so do not try to cast it.</returns>
        [System.Diagnostics.CodeAnalysis.SuppressMessage("Microsoft.Design", "CA1004:GenericMethodsShouldProvideTypeParameter")]
        public RecordReader<T> CreateRecordReader<T>(string input, Type recordReaderType)
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( recordReaderType == null )
                throw new ArgumentNullException("recordReaderType");
            if( !recordReaderType.IsSubclassOf(typeof(RecordReader<T>)) )
                throw new ArgumentException(string.Format(System.Globalization.CultureInfo.CurrentCulture, "recordReaderType does not specify a type that inherits from {0}", typeof(RecordReader<T>).FullName), "recordReaderType");

            return new RecordReaderReference<T>(this, input, recordReaderType);
        }


        /// <summary>
        /// Creates a record reader that writes data to the specified input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The directory on the DFS to write to.</param>
        /// <param name="recordWriterType">The type of the record writer to use.</param>
        /// <returns>An instance of a record writer. Note the return value is not necessarily of the type specified in <paramref name="recordWriterType"/>,
        /// so do not try to cast it.</returns>
        [System.Diagnostics.CodeAnalysis.SuppressMessage("Microsoft.Design", "CA1004:GenericMethodsShouldProvideTypeParameter")]
        public RecordWriter<T> CreateRecordWriter<T>(string output, Type recordWriterType)
        {
            return CreateRecordWriter<T>(output, recordWriterType, 0, 0);
        }

        /// <summary>
        /// Creates a record reader that writes data to the specified input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The directory on the DFS to write to.</param>
        /// <param name="recordWriterType">The type of the record writer to use.</param>
        /// <param name="blockSize">The block size of the output files, or 0 to use the DFS default block size.</param>
        /// <param name="replicationFactor">The replication factor of the output files, or 0 to use the DFS default replication factor.</param>
        /// <returns>An instance of a record writer. Note the return value is not necessarily of the type specified in <paramref name="recordWriterType"/>,
        /// so do not try to cast it.</returns>
        [System.Diagnostics.CodeAnalysis.SuppressMessage("Microsoft.Design", "CA1004:GenericMethodsShouldProvideTypeParameter")]
        public RecordWriter<T> CreateRecordWriter<T>(string output, Type recordWriterType, int blockSize, int replicationFactor)
        {
            if( output == null )
                throw new ArgumentNullException("output");
            if( recordWriterType == null )
                throw new ArgumentNullException("recordWriterType");
            if( !recordWriterType.IsSubclassOf(typeof(RecordWriter<T>)) )
                throw new ArgumentException(string.Format(System.Globalization.CultureInfo.CurrentCulture, "recordWriterType does not specify a type that inherits from {0}", typeof(RecordWriter<T>).FullName), "recordWriterType");
            if( replicationFactor < 0 )
                throw new InvalidOperationException("Replication factor may not be less than zero.");
            if( blockSize < 0 )
                throw new InvalidOperationException("Block size may not be less than zero.");

            return new RecordWriterReference<T>(this, output, recordWriterType, blockSize, replicationFactor);
        }

        /// <summary>
        /// Processes records using the specified task type.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="taskType">The type of the task.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, Type taskType)
        {
            ProcessRecords(input, output, taskType, null, null);
        }

        /// <summary>
        /// Processes records using the specified task type.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="taskType">The type of the task.</param>
        /// <param name="stageId">The ID of this processing stage in the job, or <see langword="null"/> to use the name of the task type.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, Type taskType, string stageId)
        {
            ProcessRecords(input, output, taskType, stageId, null);
        }

        /// <summary>
        /// Processes records using the specified task type.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="taskType">The type of the task.</param>
        /// <param name="stageId">The ID of this processing stage in the job, or <see langword="null"/> to use the name of the task type.</param>
        /// <param name="stageSettings">A dictionary containing settings to use for the stage. May be <see langword="null"/>.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, Type taskType, string stageId, IDictionary<string, string> stageSettings)
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( taskType == null )
                throw new ArgumentNullException("taskType");

            ProcessRecordsInternal<TInput, TOutput>(input, output, taskType, stageId, stageSettings, 0);
        }

        /// <summary>
        /// Processes records using the specified task function.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="task">The task function.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, TaskFunction<TInput, TOutput> task)
         {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( task == null )
                throw new ArgumentNullException("task");

            ProcessRecords<TInput, TOutput>(input, output, task.Method, null, 0, false, true);
        }

        /// <summary>
        /// Processes records using the specified task function.
        /// </summary>
        /// <typeparam name="TInput">The input record type.</typeparam>
        /// <typeparam name="TOutput">The output record type.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="task">The task function.</param>
        /// <param name="stageSettings">A dictionary containing settings to use for the stage. May be <see langword="null"/>.</param>
        public void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, TaskFunctionWithConfiguration<TInput, TOutput> task, IDictionary<string, string> stageSettings)
         {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( task == null )
                throw new ArgumentNullException("task");

            ProcessRecords<TInput, TOutput>(input, output, task.Method, stageSettings, 0, true, true);
        }


        /// <summary>
        /// Processes records using the specified accumulator function.
        /// </summary>
        /// <typeparam name="TKey">The type of the key of the records.</typeparam>
        /// <typeparam name="TValue">The type of the value of the records.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="accumulatorTaskType">The accumulator task type.</param>
        [System.Diagnostics.CodeAnalysis.SuppressMessage("Microsoft.Design", "CA1006:DoNotNestGenericTypesInMemberSignatures")]
        public void AccumulateRecords<TKey, TValue>(RecordReader<KeyValuePairWritable<TKey, TValue>> input, RecordWriter<KeyValuePairWritable<TKey, TValue>> output, Type accumulatorTaskType)
            where TKey : IComparable<TKey>
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( accumulatorTaskType == null )
                throw new ArgumentNullException("accumulatorTaskType");
            if( !accumulatorTaskType.IsSubclassOf(typeof(AccumulatorTask<TKey, TValue>)) )
                throw new ArgumentException("The specified task type is not an accumulator task.", "accumulatorTaskType");

            // DFS input is treated as being a single range; therefore to correctly accumulate the entire input, we must partition it.
            // If connecting directly to DFS input, we will therefore gather all into a single partition.
            // We will always connect an accumulator task to the input. If the input is another task, we pipeline it.

            RecordCollector<KeyValuePairWritable<TKey, TValue>> collector = RecordCollector<KeyValuePairWritable<TKey, TValue>>.GetCollector(input);
            if( collector == null )
            {
                // Note: as per research diary 16-10-2009, we cannot decide to match the output channel's partition count here because
                // depending on the partitioner that might change the semantics of the operation.

                // Connecting directly to DFS input, so we're creating a single partition.
                RecordCollector<KeyValuePairWritable<TKey, TValue>> intermediateCollector = new RecordCollector<KeyValuePairWritable<TKey, TValue>>(null, null, 1);
                ProcessRecords(input, intermediateCollector.CreateRecordWriter(), accumulatorTaskType, "Input" + accumulatorTaskType.Name);
                // TODO: Optimize for input with only one block (in which case this second step isn't necessary).
                ProcessRecords(intermediateCollector.CreateRecordReader(), output, accumulatorTaskType);
            }
            else
            {
                if( collector.InputStage != null )
                {
                    // TODO: With an unspecified partition count on the input stage, we could try to match the output channel partitions if the partitioner type is the same.
                    // See research diary 16-10-2009

                    RecordWriter<KeyValuePairWritable<TKey, TValue>> outputWriter = output;
                    RecordCollector<KeyValuePairWritable<TKey, TValue>> intermediateCollector = null;
                    // We'll need a second step unless:
                    // - The input channel is explicitly a pipeline channel.
                    // - There is only one input task and the output is to the DFS (in this case, if partitioning was specified it'll use internal partitioning)
                    //   We use a second step when writing to a channel because we don't want to use internal partitioning in this case.
                    //   Note we use a second step when writing to a channel even if there is only one input task and no partitioning.The second step will use
                    //   EmptyTask (no aggregation needed with one input) so it can be replaced later.
                    if( !(collector.ChannelType == ChannelType.Pipeline || (collector.InputStage.TaskCount == 1 && output is RecordWriterReference<KeyValuePairWritable<TKey, TValue>>)) )
                    {
                        // We'll need a second step, so create an intermedate collector and modify the original input channel
                        intermediateCollector = new RecordCollector<KeyValuePairWritable<TKey, TValue>>(collector.ChannelType, collector.PartitionerType, collector.Partitions);
                        collector.Partitions = 1;
                        collector.ChannelType = ChannelType.Pipeline;
                        outputWriter = intermediateCollector.CreateRecordWriter();
                    }
                    else if( collector.ChannelType == null )
                    {
                        // If no second stage is needed and the channel type is unspecified, make it a pipeline channel.
                        collector.ChannelType = ChannelType.Pipeline;
                    }

                    // Create the pipelined stage (or if the input stage had one task, and the channel type was specified to be other than pipeline, that stage)
                    ProcessRecords(input, outputWriter, accumulatorTaskType, "Input" + accumulatorTaskType.Name); // Have to specify name because it might replace rather than pipeline

                    if( intermediateCollector != null )
                    {
                        // Create the second step.
                        // If the input stage has only one task, then we don't need to aggregate the results so we can use EmptyTask. 
                        Type taskType = collector.InputStage.TaskCount == 1 ? typeof(EmptyTask<KeyValuePairWritable<TKey, TValue>>) : accumulatorTaskType;
                        ProcessRecords(intermediateCollector.CreateRecordReader(), output, taskType);
                    }
                }
                else if( collector.InputChannels != null )
                {
                    // TODO: Reconsider this. Like files, are join results considered single range or pre-partitioned?
                    // We're connecting to multiple input channels. We will leave the partitions as they are, so it's only a single-step, and we can't pipeline.
                    //ProcessRecords(input, output, accumulatorTaskType);
                    throw new NotImplementedException();
                }
                else
                    throw new ArgumentException("The specified record reader was not created by a JobBuilder or RecordCollector.", "input");
            }
        }

        /// <summary>
        /// Processes records using the specified accumulator function.
        /// </summary>
        /// <typeparam name="TKey">The type of the key of the records.</typeparam>
        /// <typeparam name="TValue">The type of the value of the records.</typeparam>
        /// <param name="input">The record reader to read records to process from.</param>
        /// <param name="output">The record writer to write the result to.</param>
        /// <param name="accumulator">The accumulator function.</param>
        [System.Diagnostics.CodeAnalysis.SuppressMessage("Microsoft.Design", "CA1006:DoNotNestGenericTypesInMemberSignatures")]
        public void AccumulateRecords<TKey, TValue>(RecordReader<KeyValuePairWritable<TKey, TValue>> input, RecordWriter<KeyValuePairWritable<TKey, TValue>> output, AccumulatorFunction<TKey, TValue> accumulator)
            where TKey : IComparable<TKey>
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( accumulator == null )
                throw new ArgumentNullException("accumulator");

            MethodInfo accumulatorMethod = accumulator.Method;
            if( !(accumulatorMethod.IsStatic && accumulatorMethod.IsPublic) )
                throw new ArgumentException("The accumulator method specified must be public and static.", "accumulator");

            CreateDynamicAssembly();

            TypeBuilder taskTypeBuilder = _dynamicModule.DefineType(_dynamicAssembly.GetName().Name + "." + accumulatorMethod.Name, TypeAttributes.Class | TypeAttributes.Sealed, typeof(AccumulatorTask<TKey, TValue>));

            SetAllowRecordReuseAttribute(accumulatorMethod, taskTypeBuilder);

            MethodBuilder accumulateMethod = taskTypeBuilder.DefineMethod("Accumulate", MethodAttributes.Public | MethodAttributes.Virtual, null, new[] { typeof(TKey), typeof(TValue), typeof(TValue) });
            accumulateMethod.DefineParameter(1, ParameterAttributes.None, "key");
            accumulateMethod.DefineParameter(2, ParameterAttributes.None, "currentValue");
            accumulateMethod.DefineParameter(3, ParameterAttributes.None, "newValue");

            ILGenerator generator = accumulateMethod.GetILGenerator();
            generator.Emit(OpCodes.Ldarg_1);
            generator.Emit(OpCodes.Ldarg_2);
            generator.Emit(OpCodes.Ldarg_3);
            generator.Emit(OpCodes.Call, accumulatorMethod);
            generator.Emit(OpCodes.Ret);

            Type taskType = taskTypeBuilder.CreateType();

            AccumulateRecords(input, output, taskType);
        }

        /// <summary>
        /// Sorts the records using the specified partitioner type.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input records to sort.</param>
        /// <param name="output">The record writer receiving the sorted result.</param>
        public void SortRecords<T>(RecordReader<T> input, RecordWriter<T> output)
        {
            SortRecords(input, output, null);
        }
        
        /// <summary>
        /// Sorts the records using the specified partitioner type.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input records to sort.</param>
        /// <param name="output">The record writer receiving the sorted result.</param>
        /// <param name="comparerType">The <see cref="IComparer{T}"/> to use to compare elements while sorting, or <see langword="null"/> to use <see cref="Comparer{T}.Default"/>.</param>
        public void SortRecords<T>(RecordReader<T> input, RecordWriter<T> output, Type comparerType)
        {
            SortRecordsInternal<T>(input, output, comparerType, false);
        }

        /// <summary>
        /// Partitions the records according to the partitioning options specified by the output channel, without processing them.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input records to partition.</param>
        /// <param name="output">The output to write the partitions to.</param>
        public void PartitionRecords<T>(RecordReader<T> input, RecordWriter<T> output)
        {
            PartitionRecords(input, output, null);
        }
        
        /// <summary>
        /// Partitions the records according to the partitioning options specified by the output channel, without processing them.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="input">The input records to partition.</param>
        /// <param name="output">The output to write the partitions to.</param>
        /// <param name="stageId">The ID of the stage.</param>
        public void PartitionRecords<T>(RecordReader<T> input, RecordWriter<T> output, string stageId)
        {
            if( RecordCollector<T>.GetCollector(output) == null )
                throw new ArgumentException("The output of a partitioning operation must be a RecordCollector.", "output");
            ProcessRecords(input, output, typeof(EmptyTask<T>), stageId);
        }

        /// <summary>
        /// Generates records using a task that takes no input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The output to write the records to.</param>
        /// <param name="taskType">The type of the task.</param>
        /// <param name="taskCount">The number of task instances to create when running the job.</param>
        public void GenerateRecords<T>(RecordWriter<T> output, Type taskType, int taskCount)
        {
            GenerateRecords(output, taskType, taskCount, null, null);
        }

        /// <summary>
        /// Generates records using a task that takes no input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The output to write the records to.</param>
        /// <param name="taskType">The type of the task.</param>
        /// <param name="taskCount">The number of task instances to create when running the job.</param>
        /// <param name="stageId">The ID of this processing stage in the job, or <see langword="null"/> to use the name of the task type.</param>
        /// <param name="stageSettings">A dictionary containing settings to use for the stage. May be <see langword="null"/>.</param>
        public void GenerateRecords<T>(RecordWriter<T> output, Type taskType, int taskCount, string stageId, IDictionary<string, string> stageSettings)
        {
            if( output == null )
                throw new ArgumentNullException("output");
            if( taskType == null )
                throw new ArgumentNullException("taskType");
            if( taskCount < 1 )
                throw new ArgumentOutOfRangeException("taskCount");

            // Even though it's not used, we still need to pass the input type to the ProcessRecordsInternal method, which we'll need to do using reflection.
            Type inputType = taskType.FindGenericInterfaceType(typeof(ITask<,>), true).GetGenericArguments()[0];
            MethodInfo processRecordsMethod = GetType().GetMethod("ProcessRecordsInternal", BindingFlags.NonPublic | BindingFlags.Instance);
            processRecordsMethod = processRecordsMethod.MakeGenericMethod(inputType, typeof(T));
            processRecordsMethod.Invoke(this, new object[] { null, output, taskType, stageId, stageSettings, taskCount });
        }

        /// <summary>
        /// Generates records using a task that takes no input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The output to write the records to.</param>
        /// <param name="task">The task function.</param>
        /// <param name="taskCount">The number of task instances to create when running the job.</param>
        public void GenerateRecords<T>(RecordWriter<T> output, OutputOnlyTaskFunction<T> task, int taskCount)
        {
            if( task == null )
                throw new ArgumentNullException("task");
            if( output == null )
                throw new ArgumentNullException("output");
            if( taskCount < 1 )
                throw new ArgumentOutOfRangeException("taskCount");

            ProcessRecords<StringWritable, T>(null, output, task.Method, null, taskCount, false, false);
        }

        /// <summary>
        /// Generates records using a task that takes no input.
        /// </summary>
        /// <typeparam name="T">The type of the records.</typeparam>
        /// <param name="output">The output to write the records to.</param>
        /// <param name="task">The task function.</param>
        /// <param name="taskCount">The number of task instances to create when running the job.</param>
        /// <param name="stageSettings">A dictionary containing settings to use for the stage. May be <see langword="null"/>.</param>
        public void GenerateRecords<T>(RecordWriter<T> output, OutputOnlyTaskFunctionWithConfiguration<T> task, int taskCount, IDictionary<string, string> stageSettings)
        {
            if( task == null )
                throw new ArgumentNullException("task");
            if( output == null )
                throw new ArgumentNullException("output");
            if( taskCount < 1 )
                throw new ArgumentOutOfRangeException("taskCount");

            ProcessRecords<StringWritable, T>(null, output, task.Method, stageSettings, taskCount, true, false);
        }

        /// <summary>
        /// 
        /// </summary>
        /// <typeparam name="TOuter"></typeparam>
        /// <typeparam name="TInner"></typeparam>
        /// <typeparam name="TResult"></typeparam>
        /// <param name="outerInput"></param>
        /// <param name="innerInput"></param>
        /// <param name="output"></param>
        /// <param name="joinRecordReader"></param>
        /// <param name="outerComparer"></param>
        /// <param name="innerComparer"></param>
        public void JoinRecords<TOuter, TInner, TResult>(RecordReader<TOuter> outerInput, RecordReader<TInner> innerInput, RecordWriter<TResult> output, Type joinRecordReader, Type outerComparer, Type innerComparer)
            where TOuter : class
            where TInner : class
            where TResult : new()
        {
            if( outerInput == null )
                throw new ArgumentNullException("outerInput");
            if( innerInput == null )
                throw new ArgumentNullException("innerInput");
            if( output == null )
                throw new ArgumentNullException("output");
            if( joinRecordReader == null )
                throw new ArgumentNullException("joinRecordReader");
            if( !joinRecordReader.IsSubclassOf(typeof(InnerJoinRecordReader<TOuter, TInner, TResult>)) )
                throw new ArgumentException("The specified join record reader type does not inherit from InnerJoinRecordReader.", "joinRecordReader");
            if( outerComparer != null && !(outerComparer.GetInterfaces().Contains(typeof(IComparer<TOuter>)) && outerComparer.GetInterfaces().Contains(typeof(IEqualityComparer<TOuter>))) )
                throw new ArgumentException("The specified outer comparer does not implement IComparer<TOuter> or IEqualityComparer<TOuter>.", "outerComparer");
            if( innerComparer != null && !(innerComparer.GetInterfaces().Contains(typeof(IComparer<TInner>)) && innerComparer.GetInterfaces().Contains(typeof(IEqualityComparer<TInner>))) )
                throw new ArgumentException("The specified inner comparer does not implement IComparer<TInner> or IEqualityComparer<TInner>.", "innerComparer");

            RecordCollector<TOuter> outerInputCollector = RecordCollector<TOuter>.GetCollector(outerInput);
            RecordCollector<TInner> innerInputCollector = RecordCollector<TInner>.GetCollector(innerInput);
            ConfigureInputCollectorForJoin(outerInputCollector, outerComparer);
            ConfigureInputCollectorForJoin(innerInputCollector, innerComparer);

            RecordCollector<TOuter> outerSortCollector = new RecordCollector<TOuter>(null, outerInputCollector == null ? null : outerInputCollector.PartitionerType, outerInputCollector == null ? 1 : outerInputCollector.Partitions);
            RecordCollector<TInner> innerSortCollector = new RecordCollector<TInner>(null, innerInputCollector == null ? null : innerInputCollector.PartitionerType, innerInputCollector == null ? 1 : innerInputCollector.Partitions);
            outerSortCollector.MultiInputRecordReaderType = typeof(MergeRecordReader<TOuter>);
            innerSortCollector.MultiInputRecordReaderType = typeof(MergeRecordReader<TInner>);

            SortRecordsInternal(outerInput, outerSortCollector.CreateRecordWriter(), outerComparer, true);
            SortRecordsInternal(innerInput, innerSortCollector.CreateRecordWriter(), innerComparer, true);

            RecordCollector<TResult> outputCollector = RecordCollector<TResult>.GetCollector(output);
            if( outputCollector == null || !((outputCollector.Partitions == null || outputCollector.Partitions == outerSortCollector.InputStage.InternalPartitionCount) && outputCollector.PartitionerType == null) )
            {
                // Writing to the DFS; we need an empty stage to do the join.
                outputCollector = new RecordCollector<TResult>();
                outputCollector.InputChannels = new[] { outerSortCollector.ToInputStageInfo(ChannelType.File), innerSortCollector.ToInputStageInfo(ChannelType.File) };
                outputCollector.MultiInputRecordReaderType = joinRecordReader;
                outputCollector.Partitions = outerSortCollector.InputStage.InternalPartitionCount;

                // TODO: Unique stage names.
                ProcessRecords(outputCollector.CreateRecordReader(), output, typeof(EmptyTask<TResult>), "JoinStage");
            }
            else
            {
                outputCollector.InputChannels = new[] { outerSortCollector.ToInputStageInfo(ChannelType.File), innerSortCollector.ToInputStageInfo(ChannelType.File) };
                outputCollector.MultiInputRecordReaderType = joinRecordReader;
                outputCollector.Partitions = outerSortCollector.InputStage.InternalPartitionCount;
            }
        }

        private static void ConfigureInputCollectorForJoin<T>(RecordCollector<T> inputCollector, Type comparer)
        {
            if( inputCollector != null )
            {
                // Set the input channel types to pipeline; this will make sure SortRecords will not add an EmptyTask second stage.
                if( inputCollector.ChannelType == null )
                {
                    inputCollector.ChannelType = ChannelType.Pipeline;
                }
                if( comparer != null )
                {
                    if( inputCollector.InputStage != null )
                        inputCollector.InputStage.AddSetting(PartitionerConstants.EqualityComparerSetting, comparer.AssemblyQualifiedName);
                    else if( inputCollector.InputChannels != null )
                    {
                        foreach( InputStageInfo info in inputCollector.InputChannels )
                        {
                            info.InputStage.AddSetting(PartitionerConstants.EqualityComparerSetting, comparer.AssemblyQualifiedName);
                        }
                    }
                    else
                        throw new ArgumentException("The specified input record collector is not being written to.");
                }
            }
        }

        private void ProcessRecordsInternal<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, Type taskType, string stageId, IDictionary<string, string> stageSettings, int taskCount)
        {
            if( stageId == null )
                stageId = taskType.Name;

            Type taskInterfaceType = taskType.FindGenericInterfaceType(typeof(ITask<,>), true);
            Type[] arguments = taskInterfaceType.GetGenericArguments();
            if( !(arguments[0] == typeof(TInput) && arguments[1] == typeof(TOutput)) )
                throw new ArgumentException("The specified task type does not have input and output types matching the specified record reader and writer.", "taskType");

            string outputPath = null;
            Type outputWriterType = null;
            RecordCollector<TOutput> collector = null;
            RecordWriterReference<TOutput> outputRef = output as RecordWriterReference<TOutput>;
            if( outputRef != null )
            {
                outputPath = outputRef.Output;
                outputWriterType = outputRef.RecordWriterType;
                _assemblies.Add(outputWriterType.Assembly);
            }
            else
            {
                collector = RecordCollector<TOutput>.GetCollector(output);
                if( collector != null )
                {
                    if( collector.InputStage != null || collector.InputChannels != null )
                        throw new ArgumentException("Cannot write to the specified record writer, because that writer is already used by another stage.", "output");
                    if( collector.PartitionerType != null )
                        _assemblies.Add(collector.PartitionerType.Assembly);
                }
                else
                    throw new ArgumentException("Unsupported output record writer.", "output");
            }

            RecordReaderReference<TInput> inputRef = input as RecordReaderReference<TInput>;
            StageConfiguration stage;
            if( inputRef != null )
            {
                // We're adding an input stage.
                stage = _job.AddInputStage(stageId, _dfsClient.NameServer.GetFileSystemEntryInfo(inputRef.Input), taskType, inputRef.RecordReaderType, outputPath, outputWriterType);
                _assemblies.Add(inputRef.RecordReaderType.Assembly);
            }
            else if( input != null )
            {
                RecordCollector<TInput> inputCollector = RecordCollector<TInput>.GetCollector(input);
                if( inputCollector == null )
                    throw new ArgumentException("The specified record reader was not created by a JobBuilder or RecordCollector.", "input");
                if( inputCollector.InputStage == null && inputCollector.InputChannels == null )
                    throw new ArgumentException("Cannot read from the specified record reader because the associated RecordCollector isn't being written to.");

                // Determine the number of partitions to use on the input channel
                taskCount = DetermineTaskCount<TInput>(taskCount, inputCollector);


                // We can replace an empty task if:
                // - The channel type is pipeline and taskCount is one.
                // - The channel type is not specified, the input stage is not a child stage, and the task count, partitioner type and multi input record 
                //   reader type are the same as the EmptyTask's input.
                if( CanReplaceEmptyTask<TInput>(taskCount, inputCollector) )
                {
                    // TODO: Under these circumstances, if the input stage task is something other than EmptyTask we could still pipeline this stage rather than use file or TCP.

                    // Replace the EmptyTask
                    stage = inputCollector.InputStage;
                    stage.TaskType = taskType;
                    _job.RenameStage(stage, stageId);
                    if( outputRef != null )
                    {
                        stage.SetDfsOutput(outputPath, outputWriterType);
                    }
                }
                else
                {
                    // We default to the File channel if not specified.
                    ChannelType channelType = inputCollector.ChannelType == null ? ChannelType.File : inputCollector.ChannelType.Value;

                    InputStageInfo[] inputStages;
                    if( inputCollector.InputStage != null )
                    {
                        inputStages = new[] { inputCollector.ToInputStageInfo(channelType) };
                    }
                    else
                        inputStages = inputCollector.InputChannels;
                    // If there is only one input, the stageMultiInputRecordReaderType parameter won't be used so it doesn't matter what we pass.
                    // If there are more than one, the multi input record reader type of this inputCollector indicates what reader to use to combine
                    // the input from the stages so we must pass that.
                    stage = _job.AddStage(stageId, taskType, taskCount, inputStages, inputCollector.MultiInputRecordReaderType, outputPath, outputWriterType);
                }
            }
            else // no input
            {
                stage = _job.AddStage(stageId, taskType, taskCount, null, outputPath, outputWriterType);
            }

            if( outputRef != null )
            {
                stage.DfsOutput.BlockSize = outputRef.BlockSize;
                stage.DfsOutput.ReplicationFactor = outputRef.ReplicationFactor;
            }

            if( stageSettings != null )
            {
                stage.StageSettings = new SettingsDictionary(stageSettings);
            }

            if( collector != null )
                collector.InputStage = stage;

            if( !object.Equals(taskType.Assembly, _dynamicAssembly) )
                _assemblies.Add(taskType.Assembly);
        }

        private int DetermineTaskCount<TInput>(int taskCount, RecordCollector<TInput> inputCollector)
        {
            if( taskCount == 0 )
            {
                if( inputCollector.Partitions != null )
                    taskCount = inputCollector.Partitions.Value; // Use specified amount
                else if( inputCollector.ChannelType == ChannelType.Pipeline )
                    taskCount = 1; // Pipeline channel always uses one if unspecified
                else if( inputCollector.InputStage != null && inputCollector.InputStage.InternalPartitionCount > 1 )
                    taskCount = inputCollector.InputStage.InternalPartitionCount; // Connecting to a compound stage with internal partitioning we must use the same number of tasks.
                else
                    taskCount = _jetClient.JobServer.GetMetrics().TaskServers.Count; // Otherwise default to the number of nodes in the cluster
            }
            return taskCount;
        }

        private bool CanReplaceEmptyTask<TInput>(int taskCount, RecordCollector<TInput> inputCollector)
        {
            // We can replace an empty task if:
            // - The channel type is pipeline and taskCount is one.
            // - The channel type is not specified, the input stage is not a child stage, and the task count, partitioner type and multi input record 
            //   reader type are the same as the EmptyTask's input.
            return inputCollector.InputStage != null && inputCollector.InputStage.TaskType == typeof(EmptyTask<TInput>) &&
                                ((inputCollector.ChannelType == ChannelType.Pipeline && taskCount == 1) ||
                                 (inputCollector.ChannelType == null && inputCollector.InputStage.Parent == null && taskCount == inputCollector.InputStage.TaskCount && MatchInputChannelSettings(inputCollector.InputStage, inputCollector.PartitionerType, inputCollector.MultiInputRecordReaderType)));
        }

        private void ProcessRecords<TInput, TOutput>(RecordReader<TInput> input, RecordWriter<TOutput> output, MethodInfo taskMethod, IDictionary<string, string> stageSettings, int taskCount, bool useConfiguration, bool useInput)
        {
            if( !(taskMethod.IsStatic && taskMethod.IsPublic) )
                throw new ArgumentException("The task method specified must be public and static.", "taskMethod");

            CreateDynamicAssembly();

            TypeBuilder taskTypeBuilder = _dynamicModule.DefineType(_dynamicAssembly.GetName().Name + "." + taskMethod.Name, TypeAttributes.Class | TypeAttributes.Public | TypeAttributes.Sealed, typeof(Configurable), new[] { typeof(IPullTask<TInput, TOutput>) });

            SetAllowRecordReuseAttribute(taskMethod, taskTypeBuilder);

            MethodBuilder runMethod = taskTypeBuilder.DefineMethod("Run", MethodAttributes.Public | MethodAttributes.Virtual, null, new[] { typeof(RecordReader<TInput>), typeof(RecordWriter<TOutput>) });
            runMethod.DefineParameter(1, ParameterAttributes.None, "input");
            runMethod.DefineParameter(2, ParameterAttributes.None, "output");

            ILGenerator generator = runMethod.GetILGenerator();
            if( useInput )
            {
                generator.Emit(OpCodes.Ldarg_1);
            }
            generator.Emit(OpCodes.Ldarg_2);
            if( useConfiguration )
            {
                // Put the TaskAttemptConfiguration on the stack.
                generator.Emit(OpCodes.Ldarg_0);
                generator.Emit(OpCodes.Call, typeof(Configurable).GetProperty("TaskAttemptConfiguration").GetGetMethod());
            }
            generator.Emit(OpCodes.Call, taskMethod);
            generator.Emit(OpCodes.Ret);

            Type taskType = taskTypeBuilder.CreateType();

            ProcessRecordsInternal(input, output, taskType, null, stageSettings, taskCount);
        }
        
        private static void SetAllowRecordReuseAttribute(MethodInfo taskMethod, TypeBuilder taskTypeBuilder)
        {
            Type allowRecordReuseAttributeType = typeof(AllowRecordReuseAttribute);
            AllowRecordReuseAttribute allowRecordReuse = (AllowRecordReuseAttribute)Attribute.GetCustomAttribute(taskMethod, allowRecordReuseAttributeType);
            if( allowRecordReuse != null )
            {
                ConstructorInfo ctor = allowRecordReuseAttributeType.GetConstructor(Type.EmptyTypes);
                PropertyInfo passThrough = allowRecordReuseAttributeType.GetProperty("PassThrough");

                CustomAttributeBuilder allowRecordReuseBuilder = new CustomAttributeBuilder(ctor, new object[] { }, new[] { passThrough }, new object[] { allowRecordReuse.PassThrough });
                taskTypeBuilder.SetCustomAttribute(allowRecordReuseBuilder);
            }
        }

        private void SaveDynamicAssembly()
        {
            string assemblyFileName = _dynamicAssembly.GetName().Name + ".dll";
            _dynamicAssembly.Save(assemblyFileName);
            _assemblySaved = true;
        }

        private void CreateDynamicAssembly()
        {
            if( _assemblySaved )
                throw new InvalidOperationException("You cannot define new delegate-based tasks after the dynamic assembly has been saved.");
            if( _dynamicAssembly == null )
            {
                // Use a Guid to ensure a unique name.
                AssemblyName name = new AssemblyName("Tkl.Jumbo.Jet.Generated." + Guid.NewGuid().ToString("N"));
                _dynamicAssemblyDir = Path.GetTempPath();
                _dynamicAssembly = AppDomain.CurrentDomain.DefineDynamicAssembly(name, AssemblyBuilderAccess.Save, _dynamicAssemblyDir);
                _dynamicModule = _dynamicAssembly.DefineDynamicModule(name.Name, name.Name + ".dll");
            }
        }

        private bool MatchInputChannelSettings(StageConfiguration stage, Type partitionerType, Type multiInputRecordReaderType)
        {
            var inputStages = _job.GetInputStagesForStage(stage.StageId).ToArray();
            if( inputStages.Length == 1 )
            {
                StageConfiguration inputStage = inputStages[0];
                return (partitionerType == null || inputStage.OutputChannel.PartitionerType.ReferencedType == partitionerType) && inputStage.OutputChannel.MultiInputRecordReaderType.ReferencedType == multiInputRecordReaderType;
            }
            else
                return false;
        }

        private void SortRecordsInternal<T>(RecordReader<T> input, RecordWriter<T> output, Type comparerType, bool forceNoMergeStage)
        {
            if( input == null )
                throw new ArgumentNullException("input");
            if( output == null )
                throw new ArgumentNullException("output");
            if( comparerType != null && !comparerType.GetInterfaces().Contains(typeof(IComparer<T>)) )
                throw new ArgumentException("The specified comparer type does not implement IComparer<T>.");

            RecordCollector<T> inputCollector = RecordCollector<T>.GetCollector(input);

            Type sortTaskType = typeof(SortTask<T>);
            Type mergeReaderType = typeof(MergeRecordReader<T>);

            ++_sortStages;
            string sortStageId = "SortStage";
            if( _sortStages > 1 )
                sortStageId += _sortStages.ToString(System.Globalization.CultureInfo.InvariantCulture);
            string mergeStageId = "MergeStage";
            if( _sortStages > 1 )
                mergeStageId += _sortStages.ToString(System.Globalization.CultureInfo.InvariantCulture);

            // TODO: Stage names.
            // DFS input is treated as being a single range; therefore to correctly sort the entire input, we must partition it.
            // If connecting directly to DFS input, we will therefore gather all into a single partition.
            // We will always connect an accumulator task to the input. If the input is another task, we pipeline it.

            SettingsDictionary sortStageSettings = null;
            if( comparerType != null )
            {
                // The merge record reader will automatically read the comparer type from the input stage as well, so we don't need to
                // specify anything else for that.
                _assemblies.Add(comparerType.Assembly);
                sortStageSettings = new SettingsDictionary();
                sortStageSettings.Add(SortTaskConstants.ComparerSetting, comparerType.AssemblyQualifiedName);
            }

            if( inputCollector == null )
            {
                if( forceNoMergeStage )
                    ProcessRecords(input, output, sortTaskType, sortStageId, sortStageSettings);
                else
                {
                    // TODO: As per research diary 16-10-2009, it might be possible to match output channel partitions.
                    // Input is DFS. We will sort it, then merge
                    RecordCollector<T> intermediateCollector = new RecordCollector<T>(null, null, 1);
                    intermediateCollector.MultiInputRecordReaderType = mergeReaderType;
                    ProcessRecords(input, intermediateCollector.CreateRecordWriter(), sortTaskType, sortStageId, sortStageSettings);
                    // We need an empty task that merges the result and writes to the output.
                    ProcessRecords(intermediateCollector.CreateRecordReader(), output, typeof(EmptyTask<T>), mergeStageId);
                }
            }
            else
            {
                if( inputCollector.InputStage != null )
                {
                    RecordWriter<T> outputWriter = output;
                    RecordCollector<T> intermediateCollector = null;
                    // We won't need a second stage if the channel is explicitly pipeline or if we have only one input.
                    // Unlike Accumulate, we don't need a second stage with one input when writing to a channel because we
                    // always use internal partitioning anyway.
                    if( !(forceNoMergeStage || inputCollector.ChannelType == ChannelType.Pipeline || inputCollector.InputStage.TaskCount == 1) )
                    {
                        intermediateCollector = new RecordCollector<T>(inputCollector.ChannelType, inputCollector.PartitionerType, inputCollector.Partitions);
                        intermediateCollector.MultiInputRecordReaderType = mergeReaderType;
                        inputCollector.ChannelType = ChannelType.Pipeline;
                        outputWriter = intermediateCollector.CreateRecordWriter();
                    }
                    else if( inputCollector.ChannelType == null )
                    {
                        // Merge not necessary, and channel type was not specified, so use Pipeline
                        inputCollector.ChannelType = ChannelType.Pipeline;
                    }
                    // TODO: As per research diary 16-10-2009, it might be possible to match output channel partitions.

                    ProcessRecords(input, outputWriter, sortTaskType, sortStageId, sortStageSettings);

                    if( intermediateCollector != null )
                    {
                        ProcessRecords(intermediateCollector.CreateRecordReader(), output, typeof(EmptyTask<T>), mergeStageId);
                    }
                }
                else if( inputCollector.InputChannels == null )
                {
                    // TODO: This'll need to work much like the DFS input case. Or maybe not; see Accumulate.
                    throw new NotImplementedException();
                }
                else
                    throw new ArgumentException("The specified record reader was not created by a JobBuilder or RecordCollector.", "input");
            }
        }    
    }
}
